# Sequence-level unlikelihood fine-tuning KoBART

## 1. Backgroud
Current Korean text abstractive generation models suffer from ```text degeneration```, which includes text repetition issue.

Applying advanced <b>stochastic decoding strategies</b> such as ```top-k sampling``` and ```nucleaus sampling``` can mitigate text repetition issue with open-ended language generation tasks. 

Recent research(Cho et al.) suggests unlikelihood training to resolve text degeneration issue.


## 2. Data Preparation

## 3. Training

## 4. Demo

## 5. Result