def sample_sequence(model, prefix_batch, prefix_length=cfg.gen.input_length,
                    continuation_length=cfg.gen.max_length,
                    top_k=cfg.gen.top_k,
                    top_p=cfg.gen.top_p):
    context = prefix_batch
    assert context['input_ids'].size(1) == prefix_length
    prev = context['input_ids']
    continuation_logits = []
    output = []
    past = None
    for i in range(continuation_length):
        out = model(prev, past_key_values=past)
        logits, past = out['logits'], out['past_key_values']
        logits = logits[:, -1, :]

        if top_k == 1 and top_p == 0:
            # greedy algo
            prev = logits.argmax(dim=1, keepdim=True)
        else:
            # random sampling
            filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)
            prev = F.softmax(filtered_logits, dim=1).multinomial(num_samples=1)

        continuation_logits.append(logits)
        output.append(prev)

    continuation_logits = torch.stack(continuation_logits, 1)
    output = torch.stack(output, 1)

    return output, continuation_logits